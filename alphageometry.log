/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
2025-07-06 19:33:17.132811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
2025-07-06 19:33:18.255998: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
I0706 19:33:18.257405 138999015860032 inference_utils.py:69] Parsing gin configuration.
I0706 19:33:18.257498 138999015860032 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0706 19:33:18.257680 138999015860032 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0706 19:33:18.257718 138999015860032 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0706 19:33:18.257738 138999015860032 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0706 19:33:18.257756 138999015860032 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0706 19:33:18.257773 138999015860032 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0706 19:33:18.257792 138999015860032 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0706 19:33:18.257810 138999015860032 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0706 19:33:18.257833 138999015860032 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=2
I0706 19:33:18.257850 138999015860032 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0706 19:33:18.257867 138999015860032 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0706 19:33:18.257909 138999015860032 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0706 19:33:18.258044 138999015860032 resource_reader.py:55] Path not found: base_htrans.gin
I0706 19:33:18.258213 138999015860032 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0706 19:33:18.258310 138999015860032 resource_reader.py:55] Path not found: trainer_configuration.gin
I0706 19:33:18.262104 138999015860032 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0706 19:33:18.262236 138999015860032 resource_reader.py:55] Path not found: size/medium_150M.gin
I0706 19:33:18.262471 138999015860032 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0706 19:33:18.262566 138999015860032 resource_reader.py:55] Path not found: options/positions_t5.gin
I0706 19:33:18.262759 138999015860032 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0706 19:33:18.262848 138999015860032 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0706 19:33:18.263136 138999015860032 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0706 19:33:18.263224 138999015860032 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0706 19:33:18.265540 138999015860032 training_loop.py:334] ==== Training loop: initializing model ====
I0706 19:33:18.268020 138999015860032 xla_bridge.py:166] Remote TPU is not linked into jax; skipping remote TPU.
I0706 19:33:18.268078 138999015860032 xla_bridge.py:413] Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'
I0706 19:33:18.268122 138999015860032 xla_bridge.py:413] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0706 19:33:18.268151 138999015860032 xla_bridge.py:413] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0706 19:33:18.268463 138999015860032 xla_bridge.py:413] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
I0706 19:33:18.268524 138999015860032 xla_bridge.py:413] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
W0706 19:33:18.268558 138999015860032 xla_bridge.py:420] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0706 19:33:18.268593 138999015860032 training_loop.py:335] Process 0 of 1
I0706 19:33:18.268619 138999015860032 training_loop.py:336] Local device count = 1
I0706 19:33:18.268643 138999015860032 training_loop.py:337] Number of replicas = 1
I0706 19:33:18.268664 138999015860032 training_loop.py:339] Using random number seed 42
I0706 19:33:18.400886 138999015860032 training_loop.py:359] Initializing the model.
I0706 19:33:18.466204 138999015860032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.466431 138999015860032 decoder_stack.py:316] dstack: scanning over 1 windows.
I0706 19:33:18.466506 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0706 19:33:18.466543 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0706 19:33:18.466572 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0706 19:33:18.466600 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0706 19:33:18.466634 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0706 19:33:18.466662 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0706 19:33:18.466690 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0706 19:33:18.466718 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0706 19:33:18.466742 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0706 19:33:18.466769 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0706 19:33:18.466794 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0706 19:33:18.466819 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0706 19:33:18.466842 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.466867 138999015860032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0706 19:33:18.466949 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.466981 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.467005 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.468019 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.471634 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.481285 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.481580 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.484246 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:18.492665 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.492723 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.492769 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.492807 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.492861 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.493521 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.493592 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.494091 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.495798 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.499603 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.500242 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.500319 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.500368 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.500402 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.500542 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.500722 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.500758 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.502734 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.502815 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.504628 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.504704 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.504920 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.513359 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.520626 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.520764 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.520987 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.521068 138999015860032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0706 19:33:18.521152 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.521186 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.521219 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.522279 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.523792 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.529703 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.530015 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.531514 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:18.534414 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.534478 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.534525 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.534564 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.534624 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.534869 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.534931 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.535173 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.535734 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.537250 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.537568 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.537649 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.537691 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.537726 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.537866 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.538061 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.538095 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.540018 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.540097 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.541647 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.541718 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.541933 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.544435 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.546416 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.546496 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.546673 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.546739 138999015860032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0706 19:33:18.546805 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.546831 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.546853 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.547821 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.549365 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.555078 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.555358 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.556869 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:18.559710 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.559767 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.559799 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.559825 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.559871 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.560121 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.560177 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.560392 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.560914 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.562266 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.562504 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.562566 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.562593 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.562625 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.562744 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.562919 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.562955 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.564888 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.564967 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.566553 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.566626 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.566831 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.569012 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.570983 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.571063 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.571279 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.571351 138999015860032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0706 19:33:18.571418 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.571445 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.571466 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.572433 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.573894 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.579796 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.580074 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.664534 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:18.667587 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.667663 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.667708 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.667737 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.667801 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.668046 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.668109 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.668374 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.668912 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.670336 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.670585 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.670649 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.670676 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.670711 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.670852 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.671048 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.671087 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.673030 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.673106 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.674674 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.674744 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.674964 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.677143 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.679099 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.679174 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.679357 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.679422 138999015860032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0706 19:33:18.679492 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.679523 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.679547 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.680513 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.682028 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.687765 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.688054 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.689508 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:18.692255 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.692308 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.692348 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.692374 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.692446 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.692676 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.692728 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.692960 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.693490 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.694932 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.695173 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.695237 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.695264 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.695297 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.695426 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.695597 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.695631 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.697822 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.697900 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.699436 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.699511 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.699723 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.701961 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.703926 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.704005 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.704191 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.704257 138999015860032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0706 19:33:18.704328 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.704355 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.704377 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.705368 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.706832 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.712581 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.712880 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.714334 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:18.717279 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.717366 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.717401 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.717427 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.717476 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.717687 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.717741 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.717958 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.718509 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.719897 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.720148 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.720213 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.720241 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.720274 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.720398 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.720563 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.720600 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.722534 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.722613 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.724137 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.724210 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.724455 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.726707 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.728698 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.728796 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.728970 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.729034 138999015860032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0706 19:33:18.729099 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.729127 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.729149 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.730339 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.731804 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.737527 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.737805 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.739266 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:18.742144 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.742201 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.742233 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.742260 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.742306 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.742523 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.742585 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.742802 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.743324 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.744760 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.744995 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.745058 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.745086 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.745118 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.745241 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.745417 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.745457 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.747430 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.747509 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.749087 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.749162 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.749374 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.751543 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.753546 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.753626 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.753797 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.753862 138999015860032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0706 19:33:18.753925 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.753952 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.753975 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.754921 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.756427 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.762053 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.762343 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.764053 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:18.766849 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.766907 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.766939 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.766966 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.767013 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.767263 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.767331 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.767556 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.768079 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.769421 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.769678 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.769742 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.769769 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.769802 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.769926 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.770098 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.770132 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.772078 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.772159 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.773642 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.773715 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.773931 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.776122 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.778118 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.778199 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.778385 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.778454 138999015860032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0706 19:33:18.778520 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.778548 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.778570 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.779527 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.780995 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.786782 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.787068 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.788638 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:18.791584 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.791642 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.791674 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.791700 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.791747 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.791964 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.792026 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.792241 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.792788 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.794157 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.794427 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.794494 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.794521 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.794553 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.794673 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.794846 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.794882 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.796917 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.797000 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.798543 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.798617 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.798856 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.801321 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.803289 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.803373 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.803563 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.803629 138999015860032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0706 19:33:18.803713 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.803741 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.803764 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.804766 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.806231 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.812013 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.812334 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.813819 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:18.816617 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.816676 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.816708 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.816734 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.816783 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.816989 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.817045 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.817259 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.817781 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.819110 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.819350 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.819416 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.819444 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.819478 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.819606 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.819844 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.819884 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.821782 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.821863 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.823416 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.823492 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.823703 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.825904 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.827875 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.827957 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.828140 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.828204 138999015860032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0706 19:33:18.828270 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.828298 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.828325 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.829255 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.830713 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.836641 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.836932 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.838433 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:18.841248 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.841305 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.841342 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.841369 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.841418 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.841631 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.841686 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.841892 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.842396 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.843713 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.843975 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.844042 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.844069 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.844100 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.844223 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.844397 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.844433 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.846301 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.846381 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.847831 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.847904 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.848101 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.850262 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.852184 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.852263 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.852442 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.852507 138999015860032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0706 19:33:18.852574 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.852603 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.852624 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.853524 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.855017 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.860548 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.860828 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.862245 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:18.864967 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.865023 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.865054 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.865079 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.865126 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.865350 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.865416 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.865632 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.866139 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.867496 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.867733 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.867795 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.867822 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.867853 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.867971 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.868134 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.868188 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.870075 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.870154 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.871608 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.871682 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.871884 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.874200 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.876130 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.876209 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.876380 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.876551 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0706 19:33:18.876597 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0706 19:33:18.876627 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0706 19:33:18.876652 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0706 19:33:18.876675 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0706 19:33:18.876698 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0706 19:33:18.876720 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0706 19:33:18.876742 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0706 19:33:18.876763 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0706 19:33:18.876785 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0706 19:33:18.876807 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0706 19:33:18.876828 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0706 19:33:18.876848 138999015860032 decoder_stack.py:344] dstack: Final layernorm.
I0706 19:33:18.878940 138999015860032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:18.919383 138999015860032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.919467 138999015860032 decoder_stack.py:333] dstack: autoregressive generator.
I0706 19:33:18.919502 138999015860032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0706 19:33:18.919584 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.919611 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.919630 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.919661 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.921044 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.926981 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.927270 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.928691 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:18.938921 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.938979 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.939034 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.939060 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.939107 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.939732 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.939805 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.940277 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.941682 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.944659 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.945277 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.945354 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.945382 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.945414 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.945532 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.945602 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.945629 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.947701 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.947782 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.949148 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.949219 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.949283 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.951505 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.953543 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.953624 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.953805 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.953869 138999015860032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0706 19:33:18.953932 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.953958 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.953979 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.954010 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.955302 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.961416 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.961691 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.963113 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:18.970571 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.970629 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.970660 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.970686 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.970735 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.970948 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.971002 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.971241 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.971685 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.973012 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.973254 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.973325 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:18.973354 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:18.973387 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.973510 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:18.973580 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:18.973608 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.975630 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.975708 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.977090 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.977161 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:18.977241 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:18.979489 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:18.981513 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.981591 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:18.981765 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.981829 138999015860032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0706 19:33:18.981892 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:18.981919 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:18.981940 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:18.981971 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.983263 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:18.989077 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.989372 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:18.990788 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:18.998308 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:18.998371 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:18.998404 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:18.998430 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.998478 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.998689 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.998745 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.998952 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:18.999393 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.000744 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.000993 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.001061 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.001089 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.001121 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.001243 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.001318 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.001346 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.003428 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.003509 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.004926 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.005000 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.005067 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.007294 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.009377 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.009460 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.009638 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.009700 138999015860032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0706 19:33:19.009762 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.009788 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.009810 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.009841 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.011120 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.017030 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.017318 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.018764 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.026448 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.026511 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.026543 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.026568 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.026615 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.026831 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.026889 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.027102 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.027544 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.028918 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.029146 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.029209 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.029237 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.029268 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.029400 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.029475 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.029504 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.031542 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.031620 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.033060 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.033132 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.033197 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.035562 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.037556 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.037635 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.037808 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.037894 138999015860032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0706 19:33:19.037961 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.037988 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.038010 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.038041 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.039344 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.045182 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.045457 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.046845 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.054202 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.054259 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.054289 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.054319 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.054367 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.054567 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.054624 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.054825 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.055264 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.056641 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.056871 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.056935 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.056963 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.056995 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.057120 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.057195 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.057227 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.059260 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.059342 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.060726 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.060798 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.060862 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.063085 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.065091 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.065169 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.065345 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.065409 138999015860032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0706 19:33:19.065471 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.065496 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.065517 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.065547 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.066854 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.072892 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.073165 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.074615 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.082004 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.082062 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.082093 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.082118 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.082164 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.082373 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.082429 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.082639 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.083075 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.084439 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.084678 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.084743 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.084770 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.084801 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.084922 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.084992 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.085019 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.087064 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.087144 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.088530 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.088602 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.088668 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.090865 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.092871 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.092951 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.093127 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.093190 138999015860032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0706 19:33:19.093253 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.093279 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.093301 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.093335 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.094612 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.100526 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.100828 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.102215 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.109738 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.109795 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.109827 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.109852 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.109898 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.110108 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.110171 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.110389 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.110860 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.112207 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.112443 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.112509 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.112536 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.112569 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.112690 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.112759 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.112787 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.114805 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.114884 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.116275 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.116352 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.116417 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.118827 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.120917 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.120999 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.121175 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.121241 138999015860032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0706 19:33:19.121304 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.121336 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.121358 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.121389 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.122685 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.128576 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.128880 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.130300 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.137631 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.137689 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.137721 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.137747 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.137793 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.138008 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.138065 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.138277 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.138720 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.140062 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.140303 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.140372 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.140400 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.140432 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.140556 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.140625 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.140653 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.142706 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.142786 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.144163 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.144236 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.144302 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.146653 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.148701 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.148779 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.148956 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.149020 138999015860032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0706 19:33:19.149085 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.149111 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.149132 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.149163 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.150463 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.156333 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.156603 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.158013 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.165461 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.165520 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.165552 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.165578 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.165626 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.165843 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.165907 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.166127 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.166579 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.167927 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.168171 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.168234 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.168261 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.168292 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.168418 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.168488 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.168516 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.170534 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.170612 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.172023 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.172096 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.172161 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.174391 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.176390 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.176468 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.176643 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.176707 138999015860032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0706 19:33:19.176770 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.176797 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.176818 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.176848 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.178170 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.184105 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.184384 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.185803 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.193095 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.193152 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.193182 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.193208 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.193253 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.193467 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.193530 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.193743 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.194180 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.195545 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.195775 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.195838 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.195866 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.195898 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.196022 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.196092 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.196120 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.198143 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.198220 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.199608 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.199681 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.199745 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.201996 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.204002 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.204081 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.204256 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.204323 138999015860032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0706 19:33:19.204388 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.204415 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.204437 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.204467 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.205760 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.211740 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.212015 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.213416 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.221026 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.221084 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.221115 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.221141 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.221187 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.221408 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.221462 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.221673 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.222112 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.223467 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.223701 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.223764 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.223790 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.223822 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.223942 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.224010 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.224038 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.226093 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.226174 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.227539 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.227612 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.227678 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.229917 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.231950 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.232030 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.232207 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.232274 138999015860032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0706 19:33:19.232344 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.232372 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.232393 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.232425 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.233739 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.239784 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.240056 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.241468 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.248840 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.248897 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.248928 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.248953 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.249001 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.249210 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.249265 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.249517 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.249969 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.251332 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.251573 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.251636 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.251662 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.251693 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.251818 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.251887 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.251915 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.253943 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.254021 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.255413 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.255484 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.255549 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.257915 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.259909 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.259988 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.260162 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.260229 138999015860032 decoder_stack.py:344] dstack: Final layernorm.
I0706 19:33:19.261851 138999015860032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:19.305835 138999015860032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.305952 138999015860032 decoder_stack.py:333] dstack: autoregressive generator.
I0706 19:33:19.306018 138999015860032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0706 19:33:19.306082 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.306109 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.306130 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.306161 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.307515 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.313391 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.313668 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.315025 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.322197 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.322256 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.322288 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.322313 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.322367 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.322586 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.322639 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.322901 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.323332 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.324666 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.324902 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.324964 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.324990 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.325022 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.325143 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.325211 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.325239 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.327233 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.327311 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.328708 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.328801 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.328869 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.331098 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.333098 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.333176 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.333351 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.333414 138999015860032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0706 19:33:19.333477 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.333502 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.333523 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.333555 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.334837 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.340672 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.340977 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.342371 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.349596 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.349654 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.349686 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.349712 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.349760 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.349968 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.350023 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.350227 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.350650 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.352133 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.352360 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.352424 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.352450 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.352482 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.352603 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.352672 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.352699 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.354734 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.354814 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.356195 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.356267 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.356336 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.358548 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.360596 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.360675 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.360849 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.360912 138999015860032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0706 19:33:19.360972 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.360999 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.361019 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.361049 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.362328 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.368235 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.368515 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.369916 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.377158 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.377219 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.377268 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.377295 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.377357 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.377563 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.377616 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.377816 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.378234 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.379584 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.379812 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.379874 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.379901 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.379933 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.380050 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.380118 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.380145 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.382129 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.382208 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.383626 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.383699 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.383766 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.385951 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.388058 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.388136 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.388310 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.388378 138999015860032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0706 19:33:19.388441 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.388467 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.388489 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.388520 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.389822 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.395690 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.395968 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.397377 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.404649 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.404705 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.404736 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.404760 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.404806 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.405011 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.405071 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.405281 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.405705 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.407059 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.407281 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.407347 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.407374 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.407406 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.407555 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.407632 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.407662 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.409640 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.409716 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.411073 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.411140 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.411205 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.413450 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.415480 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.415558 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.415728 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.415795 138999015860032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0706 19:33:19.415858 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.415884 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.415904 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.415933 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.417240 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.423263 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.423537 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.424943 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.432203 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.432263 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.432294 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.432324 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.432372 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.432588 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.432643 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.432857 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.433289 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.434630 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.434859 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.434921 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.434947 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.434978 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.435097 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.435163 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.435190 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.437157 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.437234 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.438674 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.438747 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.438812 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.441030 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.443093 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.443171 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.443349 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.443417 138999015860032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0706 19:33:19.443481 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.443507 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.443527 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.443557 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.444881 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.450716 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.450988 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.452460 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.459937 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.459997 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.460027 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.460051 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.460098 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.460311 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.460379 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.460589 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.461021 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.462582 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.462817 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.462879 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.462904 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.462935 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.463052 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.463118 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.463145 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.465133 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.465209 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.466605 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.466675 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.466739 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.469169 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.471182 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.471259 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.471430 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.471495 138999015860032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0706 19:33:19.471559 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.471585 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.471606 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.471636 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.472935 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.479244 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.479619 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.481204 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.489136 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.489205 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.489238 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.489264 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.489337 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.489582 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.489653 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.489882 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.490351 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.491823 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.492082 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.492150 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.492178 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.492213 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.492370 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.492467 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.492500 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.494644 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.494736 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.496173 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.496251 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.496370 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.498964 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.501387 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.501482 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.501675 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.501749 138999015860032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0706 19:33:19.501817 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.501847 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.501868 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.501904 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.503296 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.509676 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.509978 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.511475 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.519289 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.519360 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.519393 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.519419 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.519471 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.519688 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.519750 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.519976 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.520443 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.521856 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.522100 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.522170 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.522200 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.522235 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.522369 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.522442 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.522472 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.524619 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.524707 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.526143 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.526218 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.526286 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.528654 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.531233 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.531340 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.531549 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.531628 138999015860032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0706 19:33:19.531699 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.531729 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.531752 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.531788 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.533174 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.539658 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.539958 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.541448 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.549655 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.549728 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.549762 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.549788 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.549842 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.550070 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.550133 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.550361 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.550823 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.552241 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.552489 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.552558 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.552587 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.552621 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.552752 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.552825 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.552854 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.555069 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.555183 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.556724 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.556807 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.556878 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.559214 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.561499 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.561591 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.561779 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.561851 138999015860032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0706 19:33:19.561919 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.561949 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.561971 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.562005 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.563370 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.569752 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.570049 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.571527 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.579694 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.579765 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.579801 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.579828 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.579879 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.580105 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.580176 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.580411 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.580874 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.582447 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.582695 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.582762 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.582789 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.582823 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.582952 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.583024 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.583054 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.585310 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.585409 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.586901 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.586981 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.587049 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.589384 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.591630 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.591726 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.591919 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.591992 138999015860032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0706 19:33:19.592058 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.592087 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.592109 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.592141 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.593501 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.599618 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.599927 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.601413 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.609198 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.609268 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.609301 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.609333 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.609386 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.609610 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.609676 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.609889 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.610352 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.611776 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.612032 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.612100 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.612128 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.612164 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.612292 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.612375 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.612405 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.614520 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.614606 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.616059 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.616138 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.616210 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.618561 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.620761 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.620846 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.621032 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.621103 138999015860032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0706 19:33:19.621169 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:19.621199 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:19.621222 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:19.621255 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.622593 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:19.628726 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.629017 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:19.630482 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:19.638282 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:19.638353 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:19.638386 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:19.638414 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.638463 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.638683 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.638744 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.638956 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.639416 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.640844 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.641092 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.641159 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:19.641188 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:19.641221 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.641351 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:19.641431 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:19.641462 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.643545 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.643628 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.645079 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.645155 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:19.645225 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:19.647546 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:19.649615 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.649700 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:19.649884 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:19.649958 138999015860032 decoder_stack.py:344] dstack: Final layernorm.
I0706 19:33:19.651661 138999015860032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:28.988700 138999015860032 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/optim/base.py:49: DeprecationWarning: Use `optax` instead of `flax.optim`. Refer to the update guide https://flax.readthedocs.io/en/latest/howtos/optax_update_guide.html for detailed instructions.
  warnings.warn(
I0706 19:33:29.171454 138999015860032 training_loop.py:409] No working directory specified.
I0706 19:33:29.171624 138999015860032 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0706 19:33:29.172211 138999015860032 checkpoints.py:429] Restoring checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0706 19:33:31.256284 138999015860032 training_loop.py:447] Only restoring trainable parameters.
I0706 19:33:31.256864 138999015860032 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0706 19:33:31.256965 138999015860032 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.257015 138999015860032 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0706 19:33:31.257060 138999015860032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0706 19:33:31.257089 138999015860032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257114 138999015860032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.257138 138999015860032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257162 138999015860032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257185 138999015860032 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0706 19:33:31.257207 138999015860032 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0706 19:33:31.257230 138999015860032 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257252 138999015860032 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.257276 138999015860032 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0706 19:33:31.257298 138999015860032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0706 19:33:31.257325 138999015860032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257350 138999015860032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.257371 138999015860032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257396 138999015860032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257417 138999015860032 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0706 19:33:31.257440 138999015860032 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0706 19:33:31.257461 138999015860032 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257484 138999015860032 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.257506 138999015860032 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0706 19:33:31.257528 138999015860032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0706 19:33:31.257550 138999015860032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257572 138999015860032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.257594 138999015860032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257616 138999015860032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257639 138999015860032 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0706 19:33:31.257661 138999015860032 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0706 19:33:31.257683 138999015860032 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257704 138999015860032 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.257725 138999015860032 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0706 19:33:31.257747 138999015860032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0706 19:33:31.257771 138999015860032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257792 138999015860032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.257815 138999015860032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257836 138999015860032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257857 138999015860032 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0706 19:33:31.257878 138999015860032 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0706 19:33:31.257900 138999015860032 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.257922 138999015860032 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.257945 138999015860032 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0706 19:33:31.257966 138999015860032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0706 19:33:31.257988 138999015860032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258010 138999015860032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.258031 138999015860032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258052 138999015860032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258073 138999015860032 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0706 19:33:31.258095 138999015860032 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0706 19:33:31.258116 138999015860032 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258138 138999015860032 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.258159 138999015860032 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0706 19:33:31.258181 138999015860032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0706 19:33:31.258204 138999015860032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258225 138999015860032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.258247 138999015860032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258269 138999015860032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258289 138999015860032 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0706 19:33:31.258310 138999015860032 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0706 19:33:31.258338 138999015860032 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258360 138999015860032 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.258381 138999015860032 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0706 19:33:31.258401 138999015860032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0706 19:33:31.258422 138999015860032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258444 138999015860032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.258467 138999015860032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258489 138999015860032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258510 138999015860032 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0706 19:33:31.258533 138999015860032 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0706 19:33:31.258555 138999015860032 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258576 138999015860032 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.258598 138999015860032 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0706 19:33:31.258621 138999015860032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0706 19:33:31.258645 138999015860032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258667 138999015860032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.258690 138999015860032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258711 138999015860032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258733 138999015860032 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0706 19:33:31.258755 138999015860032 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0706 19:33:31.258778 138999015860032 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258799 138999015860032 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.258822 138999015860032 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0706 19:33:31.258843 138999015860032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0706 19:33:31.258865 138999015860032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258888 138999015860032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.258909 138999015860032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258931 138999015860032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.258952 138999015860032 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0706 19:33:31.258973 138999015860032 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0706 19:33:31.258994 138999015860032 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.259016 138999015860032 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.259038 138999015860032 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0706 19:33:31.259059 138999015860032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0706 19:33:31.259081 138999015860032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.259103 138999015860032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.259124 138999015860032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.259145 138999015860032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.259166 138999015860032 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0706 19:33:31.259189 138999015860032 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0706 19:33:31.259210 138999015860032 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.259230 138999015860032 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.259252 138999015860032 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0706 19:33:31.259272 138999015860032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0706 19:33:31.259294 138999015860032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.259317 138999015860032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.259339 138999015860032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.259361 138999015860032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.259382 138999015860032 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0706 19:33:31.259404 138999015860032 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0706 19:33:31.259427 138999015860032 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.259449 138999015860032 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.259472 138999015860032 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0706 19:33:31.259494 138999015860032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0706 19:33:31.259515 138999015860032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.259537 138999015860032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.259559 138999015860032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.259581 138999015860032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.259602 138999015860032 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0706 19:33:31.259623 138999015860032 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0706 19:33:31.259645 138999015860032 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0706 19:33:31.259668 138999015860032 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0706 19:33:31.259686 138999015860032 training_loop.py:725] Total parameters: 152072288
I0706 19:33:31.259863 138999015860032 training_loop.py:739] Total state size: 0
I0706 19:33:31.366555 138999015860032 training_loop.py:492] Training loop: creating task for mode beam_search
I0706 19:33:31.366784 138999015860032 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0706 19:33:31.367288 138999015860032 training_loop.py:652] Compiling mode beam_search with jit.
I0706 19:33:31.367581 138999015860032 training_loop.py:89] registering functions: dict_keys([])
I0706 19:33:31.370423 138999015860032 graph.py:555] Starting to Build problem.
I0706 19:33:31.370487 138999015860032 graph.py:557] orthocenter
I0706 19:33:31.370523 138999015860032 graph.py:558] a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b ? perp a d b c
I0706 19:33:31.370987 138999015860032 graph.py:581] True
I0706 19:33:31.372724 138999015860032 ddar.py:95] Level 1/1000
I0706 19:33:31.374336 138999015860032 ddar.py:60] Depth 1/1000 time = 0.0015418529510498047
I0706 19:33:31.376317 138999015860032 ddar.py:60] Depth 2/1000 time = 0.001913309097290039
I0706 19:33:31.376543 138999015860032 alphageometry.py:281] DD+AR failed to solve the problem.
I0706 19:33:31.376588 138999015860032 alphageometry.py:599] Depth 0. There are 1 nodes to expand:
I0706 19:33:31.376616 138999015860032 alphageometry.py:603] {S} a : ; b : ; c : ; d : T a b c d 00 T a c b d 01 ? T a d b c {F1} x00
I0706 19:33:31.376638 138999015860032 alphageometry.py:608] Decoding from {S} a : ; b : ; c : ; d : T a b c d 00 T a c b d 01 ? T a d b c {F1} x00
I0706 19:33:31.413476 138999015860032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.413659 138999015860032 decoder_stack.py:316] dstack: scanning over 1 windows.
I0706 19:33:31.413727 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0706 19:33:31.413762 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0706 19:33:31.413791 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0706 19:33:31.413818 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0706 19:33:31.413845 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0706 19:33:31.413871 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0706 19:33:31.413898 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0706 19:33:31.413924 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0706 19:33:31.413949 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0706 19:33:31.413976 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0706 19:33:31.414003 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0706 19:33:31.414027 138999015860032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0706 19:33:31.414050 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.414078 138999015860032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0706 19:33:31.414142 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.414172 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.414193 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.415178 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.416719 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.423050 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.423368 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.424858 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:31.427901 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.427983 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.428018 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.428045 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.428103 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.428353 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.428429 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.428655 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.429189 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.430614 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.430870 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.430937 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.430964 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.431000 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.431127 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.431323 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.431367 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.434102 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.434223 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.435809 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.435900 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.436118 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.438552 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.440735 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.440823 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.441018 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.441091 138999015860032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0706 19:33:31.441157 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.441184 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.441207 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.442138 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.443557 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.449864 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.450157 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.451605 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:31.454598 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.454660 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.454693 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.454721 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.454771 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.454998 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.455065 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.455283 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.455809 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.457161 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.457430 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.457506 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.457536 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.457571 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.457700 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.457873 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.457909 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.460065 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.460150 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.461598 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.461675 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.461881 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.464277 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.466389 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.466470 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.466660 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.466728 138999015860032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0706 19:33:31.466794 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.466822 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.466844 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.467737 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.469137 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.475419 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.475709 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.477143 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:31.480048 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.480108 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.480142 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.480169 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.480221 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.480446 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.480504 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.480720 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.481230 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.482516 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.482753 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.482815 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.482841 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.482872 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.482994 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.483156 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.483192 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.485217 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.485296 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.486762 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.486841 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.487036 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.489340 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.491380 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.491459 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.491630 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.491699 138999015860032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0706 19:33:31.491766 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.491793 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.491813 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.492675 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.494013 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.499908 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.500180 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.501578 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:31.504380 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.504436 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.504468 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.504493 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.504541 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.504741 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.504796 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.504999 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.505500 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.506787 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.507012 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.507076 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.507103 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.507136 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.507260 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.507424 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.507460 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.509493 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.509570 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.510980 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.511052 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.511245 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.513662 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.515693 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.515772 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.515949 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.516014 138999015860032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0706 19:33:31.516079 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.516106 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.516128 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.516990 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.518323 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.524238 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.524519 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.525911 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:31.528713 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.528772 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.528804 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.528831 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.528878 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.529091 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.529154 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.529370 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.529873 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.531203 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.531441 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.531510 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.531537 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.531569 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.531693 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.531855 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.531889 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.533965 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.534045 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.535484 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.535557 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.535750 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.538048 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.540094 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.540175 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.540364 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.540432 138999015860032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0706 19:33:31.540495 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.540521 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.540541 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.541419 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.542762 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.548754 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.549022 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.550433 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:31.553381 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.553438 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.553470 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.553496 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.553544 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.553754 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.553819 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.554030 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.554537 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.555863 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.556102 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.556169 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.556196 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.556228 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.556356 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.556520 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.556556 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.558625 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.558711 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.560156 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.560230 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.560433 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.562710 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.564780 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.564864 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.565047 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.565112 138999015860032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0706 19:33:31.565175 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.565201 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.565222 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.566071 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.567436 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.573427 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.573701 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.575092 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:31.577955 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.578012 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.578044 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.578070 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.578118 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.578332 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.578390 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.578598 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.579095 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.580395 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.580642 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.580709 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.580736 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.580768 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.580889 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.581049 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.581084 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.583133 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.583212 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.584638 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.584711 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.584904 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.587223 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.589259 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.589344 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.589524 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.589590 138999015860032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0706 19:33:31.589653 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.589681 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.589702 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.590566 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.591914 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.598050 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.598321 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.599787 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:31.602837 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.602895 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.602925 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.602952 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.602998 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.603204 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.603259 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.603461 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.603978 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.605266 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.605503 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.605567 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.605593 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.605624 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.605741 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.605898 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.605932 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.607982 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.608062 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.609465 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.609539 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.609737 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.612031 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.614083 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.614162 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.614345 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.614412 138999015860032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0706 19:33:31.614474 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.614500 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.614522 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.615377 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.616757 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.622932 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.623223 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.624648 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:31.627580 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.627639 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.627671 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.627698 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.627750 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.627959 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.628022 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.628234 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.628754 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.630053 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.630311 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.630379 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.630406 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.630439 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.630583 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.630747 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.630781 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.632847 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.632929 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.634335 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.634406 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.634603 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.636935 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.639128 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.639211 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.639395 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.639461 138999015860032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0706 19:33:31.639524 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.639551 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.639573 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.640406 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.641798 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.743187 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.743592 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.745067 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:31.747951 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.748019 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.748051 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.748077 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.748132 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.748381 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.748446 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.748656 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.749163 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.750567 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.750809 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.750875 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.750900 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.750933 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.751062 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.751235 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.751269 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.753364 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.753444 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.754905 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.754977 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.755176 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.757467 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.759554 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.759639 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.759843 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.759915 138999015860032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0706 19:33:31.759977 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.760003 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.760024 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.760938 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.762276 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.768252 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.768565 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.770015 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:31.772855 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.772910 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.772939 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.772964 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.773009 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.773208 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.773275 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.773482 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.774003 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.775490 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.775719 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.775781 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.775808 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.775839 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.775955 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.776115 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.776155 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.778203 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.778278 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.779731 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.779800 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.779993 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.782357 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.784452 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.784529 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.784707 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.784774 138999015860032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0706 19:33:31.784839 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.784867 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.784890 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.785748 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.787103 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.793071 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.793371 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.794790 138999015860032 transformer_layer.py:213] tlayer: windowed attention.
I0706 19:33:31.797623 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.797677 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.797721 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.797746 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.797798 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.798012 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.798068 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.798285 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.798789 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.800145 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.800388 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.800456 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.800483 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.800515 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.800643 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.800822 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.800857 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.802956 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.803035 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.804500 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.804573 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.804775 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.807109 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.809217 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.809320 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.809512 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.809675 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0706 19:33:31.809719 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0706 19:33:31.809748 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0706 19:33:31.809774 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0706 19:33:31.809798 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0706 19:33:31.809821 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0706 19:33:31.809844 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0706 19:33:31.809865 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0706 19:33:31.809888 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0706 19:33:31.809911 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0706 19:33:31.809933 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0706 19:33:31.809956 138999015860032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0706 19:33:31.809977 138999015860032 decoder_stack.py:344] dstack: Final layernorm.
I0706 19:33:31.811662 138999015860032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0706 19:33:31.851190 138999015860032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.851284 138999015860032 decoder_stack.py:333] dstack: autoregressive generator.
I0706 19:33:31.851326 138999015860032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0706 19:33:31.851406 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.851436 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.851457 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.851489 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.852865 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.858609 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.858902 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.860248 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:31.867882 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.867940 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.867972 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.867998 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.868065 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.868294 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.868356 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.868583 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.869020 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.870348 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.870597 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.870660 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.870686 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.870744 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.870869 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.870939 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.870968 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.872980 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.873057 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.874498 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.874571 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.874638 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.876966 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.879080 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.879160 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.879352 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.879418 138999015860032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0706 19:33:31.879481 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.879509 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.879532 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.879563 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.880890 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.887053 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.887337 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.888777 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:31.896409 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.896468 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.896499 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.896525 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.896574 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.896784 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.896841 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.897049 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.897492 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.898854 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.899084 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.899149 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.899178 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.899210 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.899340 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.899413 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.899442 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.901708 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.901793 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.903245 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.903324 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.903393 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.905671 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.907772 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.907852 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.908035 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.908102 138999015860032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0706 19:33:31.908165 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.908192 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.908214 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.908244 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.909572 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.915681 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.915970 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.917443 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:31.925126 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.925192 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.925225 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.925252 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.925302 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.925521 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.925579 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.925819 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.926255 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.927688 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.927929 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.927996 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.928024 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.928058 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.928188 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.928261 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.928289 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.930490 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.930572 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.932093 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.932173 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.932242 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.934665 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.936841 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.936925 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.937112 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.937207 138999015860032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0706 19:33:31.937273 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.937302 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.937330 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.937365 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.938716 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.945123 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.945416 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.946915 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:31.954700 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.954762 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.954795 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.954822 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.954871 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.955092 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.955156 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.955378 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.955825 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.957247 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.957502 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.957571 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.957599 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.957632 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.957756 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.957827 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.957855 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.959945 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.960033 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.961493 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.961567 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.961634 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.963991 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.966087 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.966167 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.966359 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.966425 138999015860032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0706 19:33:31.966491 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.966519 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.966542 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.966573 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.967916 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:31.973994 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.974276 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:31.975733 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:31.983429 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:31.983488 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:31.983519 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:31.983545 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.983593 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.983809 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.983875 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.984095 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.984546 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.985918 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.986147 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.986213 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:31.986243 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:31.986276 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.986409 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:31.986483 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:31.986512 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.988619 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.988699 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.990150 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.990224 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:31.990290 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:31.992587 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:31.994730 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.994812 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:31.995000 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.995066 138999015860032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0706 19:33:31.995131 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:31.995159 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:31.995181 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:31.995212 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:31.996548 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.002604 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.002887 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.004397 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.011906 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.011965 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.011997 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.012025 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.012075 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.012304 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.012364 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.012571 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.012992 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.014371 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.014601 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.014666 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.014694 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.014727 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.014853 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.014923 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.014951 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.017187 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.017272 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.018728 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.018804 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.018888 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.021204 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.023281 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.023367 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.023556 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.023626 138999015860032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0706 19:33:32.023693 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.023721 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.023743 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.023774 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.025123 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.031326 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.031628 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.033072 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.040713 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.040770 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.040801 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.040830 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.040891 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.041106 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.041162 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.041381 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.041825 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.043236 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.043510 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.043576 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.043604 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.043636 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.043761 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.043831 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.043860 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.045942 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.046020 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.047446 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.047519 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.047588 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.049961 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.052059 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.052141 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.052331 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.052399 138999015860032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0706 19:33:32.052463 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.052492 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.052514 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.052545 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.053894 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.060026 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.060328 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.061763 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.068995 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.069050 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.069082 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.069108 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.069155 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.069369 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.069426 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.069635 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.070080 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.071409 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.071640 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.071701 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.071727 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.071758 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.071877 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.071944 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.071972 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.073984 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.074061 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.075516 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.075586 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.075651 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.077833 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.079832 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.079909 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.080091 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.080154 138999015860032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0706 19:33:32.080218 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.080244 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.080264 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.080296 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.081633 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.087528 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.087796 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.089188 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.096539 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.096594 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.096624 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.096650 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.096697 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.096900 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.096962 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.097167 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.097594 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.098923 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.099146 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.099209 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.099235 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.099266 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.099394 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.099492 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.099522 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.101556 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.101633 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.103020 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.103091 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.103154 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.105379 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.107397 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.107473 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.107645 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.107711 138999015860032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0706 19:33:32.107774 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.107801 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.107821 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.107852 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.109138 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.115077 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.115351 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.116753 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.124004 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.124068 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.124102 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.124127 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.124173 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.124373 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.124427 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.124624 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.125053 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.126485 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.126717 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.126780 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.126808 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.126841 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.126965 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.127034 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.127066 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.129281 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.129378 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.130803 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.130873 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.130947 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.133262 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.135284 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.135377 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.135565 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.135629 138999015860032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0706 19:33:32.135707 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.135736 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.135769 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.135801 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.137213 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.143323 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.143609 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.145057 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.152661 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.152720 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.152776 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.152802 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.152850 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.153071 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.153135 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.153362 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.153801 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.155196 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.155441 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.155507 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.155534 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.155566 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.155690 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.155759 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.155787 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.157888 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.157969 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.159422 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.159494 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.159560 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.161884 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.163986 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.164066 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.164255 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.164324 138999015860032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0706 19:33:32.164390 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.164418 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.164439 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.164471 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.165833 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.172050 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.172360 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.173844 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.181339 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.181396 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.181427 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.181452 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.181499 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.181703 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.181758 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.181963 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.182390 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.183818 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.184049 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.184112 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.184140 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.184173 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.184298 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.184371 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.184401 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.186478 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.186557 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.188011 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.188081 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.188148 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.190473 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.192572 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.192652 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.192868 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.192941 138999015860032 decoder_stack.py:344] dstack: Final layernorm.
I0706 19:33:32.194649 138999015860032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0706 19:33:32.237135 138999015860032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.237267 138999015860032 decoder_stack.py:333] dstack: autoregressive generator.
I0706 19:33:32.237308 138999015860032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0706 19:33:32.237403 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.237432 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.237452 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.237505 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.238883 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.244945 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.245240 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.246724 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.254064 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.254121 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.254150 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.254199 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.254247 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.254462 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.254516 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.254717 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.255143 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.256514 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.256741 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.256803 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.256830 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.256862 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.256982 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.257052 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.257080 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.259155 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.259233 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.260684 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.260757 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.260833 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.263103 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.265259 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.265346 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.265535 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.265602 138999015860032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0706 19:33:32.265666 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.265694 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.265716 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.265746 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.267096 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.273586 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.273881 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.275404 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.282989 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.283052 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.283086 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.283111 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.283160 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.283386 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.283443 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.283663 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.284112 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.285572 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.285811 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.285874 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.285902 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.285935 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.286061 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.286131 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.286160 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.288264 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.288351 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.289798 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.289872 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.289938 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.292298 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.294409 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.294490 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.294677 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.294744 138999015860032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0706 19:33:32.294809 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.294836 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.294858 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.294889 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.296252 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.302401 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.302670 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.304084 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.311803 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.311862 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.311892 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.311920 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.311968 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.312172 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.312225 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.312432 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.312869 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.314228 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.314457 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.314521 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.314547 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.314579 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.314700 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.314769 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.314796 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.316843 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.316921 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.318329 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.318400 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.318471 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.320770 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.322813 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.322892 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.323070 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.323135 138999015860032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0706 19:33:32.323195 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.323221 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.323242 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.323273 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.324571 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.330747 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.331023 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.332455 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.339906 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.339964 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.339995 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.340019 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.340066 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.340264 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.340323 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.340532 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.340965 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.342359 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.342601 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.342665 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.342693 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.342725 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.342846 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.342917 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.342947 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.345179 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.345257 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.346687 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.346758 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.346824 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.349085 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.351131 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.351209 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.351393 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.351475 138999015860032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0706 19:33:32.351542 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.351573 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.351594 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.351624 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.352934 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.358891 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.359161 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.360578 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.367976 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.368050 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.368080 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.368105 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.368150 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.368359 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.368416 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.368616 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.369047 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.370423 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.370648 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.370711 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.370738 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.370769 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.370893 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.370964 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.370992 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.373001 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.373090 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.374496 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.374569 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.374633 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.376857 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.378902 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.378979 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.379165 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.379230 138999015860032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0706 19:33:32.379292 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.379324 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.379349 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.379380 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.380667 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.386790 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.387064 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.388507 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.395808 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.395864 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.395894 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.395919 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.395965 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.396172 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.396237 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.396454 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.396878 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.398226 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.398461 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.398523 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.398550 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.398593 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.398715 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.398782 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.398810 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.400847 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.400930 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.402405 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.402489 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.402560 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.405019 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.407235 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.407350 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.407553 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.407656 138999015860032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0706 19:33:32.407725 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.407756 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.407779 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.407814 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.409230 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.415653 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.415968 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.417499 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.425579 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.425650 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.425684 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.425711 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.425765 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.426000 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.426074 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.426302 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.426771 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.428230 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.428514 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.428585 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.428614 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.428651 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.428794 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.428869 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.428898 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.431094 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.431184 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.432787 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.432949 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.433079 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.437550 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.440614 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.440817 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.441087 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.441196 138999015860032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0706 19:33:32.441280 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.441310 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.441342 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.441388 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.443052 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.450453 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.450917 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.452650 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.462355 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.462501 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.462546 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.462575 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.462646 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.462910 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.462988 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.463239 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.463771 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.465629 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.466060 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.466172 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.466214 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.466269 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.466495 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.466622 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.466670 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.471442 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.471713 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.474575 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.474693 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.474791 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.477581 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.479918 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.480038 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.480256 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.480344 138999015860032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0706 19:33:32.480419 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.480453 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.480479 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.480520 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.481937 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.488279 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.488660 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.490212 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.498112 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.498214 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.498257 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.498288 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.498357 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.498611 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.498683 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.498924 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.499423 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.500910 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.501184 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.501257 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.501291 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.501337 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.501479 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.501560 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.501593 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.503852 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.503960 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.505507 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.505594 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.505675 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.508099 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.510234 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.510326 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.510532 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.510611 138999015860032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0706 19:33:32.510684 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.510716 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.510742 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.510782 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.512165 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.518638 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.519018 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.520566 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.528353 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.528429 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.528470 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.528502 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.528562 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.528795 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.528864 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.529100 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.529576 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.531116 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.531385 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.531458 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.531494 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.531535 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.531671 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.531750 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.531785 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.533966 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.534056 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.535584 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.535665 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.535747 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.538190 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.540388 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.540504 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.540713 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.540800 138999015860032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0706 19:33:32.540876 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.540908 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.540934 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.540975 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.542384 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.548516 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.548820 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.550291 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.558236 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.558304 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.558355 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.558389 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.558444 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.558681 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.558743 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.558974 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.559450 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.560901 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.561167 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.561238 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.561271 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.561310 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.561500 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.561614 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.561653 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.563776 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.563857 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.565370 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.565446 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.565514 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.567882 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.570011 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.570092 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.570284 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.570357 138999015860032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0706 19:33:32.570425 138999015860032 transformer_layer.py:154] tlayer: recurrent = False
I0706 19:33:32.570452 138999015860032 transformer_layer.py:155] tlayer: compute_importance = False
I0706 19:33:32.570473 138999015860032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0706 19:33:32.570506 138999015860032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.571941 138999015860032 transformer_base.py:161] kvq: pre_attn dropout.
I0706 19:33:32.578135 138999015860032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.578435 138999015860032 transformer_base.py:194] kvq: normalize keys, queries.
I0706 19:33:32.579914 138999015860032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0706 19:33:32.587503 138999015860032 transformer_layer.py:299] tlayer: num_windows = 1.
I0706 19:33:32.587565 138999015860032 attention.py:418] Single window, no scan.
I0706 19:33:32.587597 138999015860032 transformer_layer.py:389] tlayer: self-attention.
I0706 19:33:32.587623 138999015860032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.587671 138999015860032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.587880 138999015860032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.587936 138999015860032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.588148 138999015860032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.588615 138999015860032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.590058 138999015860032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.590293 138999015860032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.590361 138999015860032 transformer_layer.py:468] tlayer: End windows.
I0706 19:33:32.590390 138999015860032 transformer_layer.py:472] tlayer: final FFN.
I0706 19:33:32.590422 138999015860032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.590548 138999015860032 transformer_base.py:410] tbase: post-attention MLP.
I0706 19:33:32.590621 138999015860032 nn_components.py:325] mlp: activation = None
I0706 19:33:32.590651 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.592859 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.592942 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.594396 138999015860032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.594469 138999015860032 transformer_base.py:443] tbase: final FFN
I0706 19:33:32.594537 138999015860032 nn_components.py:320] mlp: hidden 4096, relu
I0706 19:33:32.596828 138999015860032 nn_components.py:329] mlp: final activation = None
I0706 19:33:32.598822 138999015860032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.598904 138999015860032 nn_components.py:261] mlp: residual
I0706 19:33:32.599096 138999015860032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:33:32.599168 138999015860032 decoder_stack.py:344] dstack: Final layernorm.
I0706 19:33:32.600843 138999015860032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0706 19:34:02.474323 138999015860032 graph.py:555] Starting to Build problem.
I0706 19:34:02.475144 138999015860032 graph.py:581] True
I0706 19:34:02.475624 138999015860032 graph.py:555] Starting to Build problem.
I0706 19:34:02.476104 138999015860032 graph.py:581] True
I0706 19:34:02.476466 138999015860032 alphageometry.py:625] LM output (score=-1.119110): "e : C a c e 02 C b d e 03 ;"
I0706 19:34:02.476514 138999015860032 alphageometry.py:626] Translation: "e = on_line e a c, on_line e b d"

I0706 19:34:02.476545 138999015860032 alphageometry.py:635] Solving: "a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b; e = on_line e a c, on_line e b d ? perp a d b c"
I0706 19:34:02.476608 138999015860032 graph.py:555] Starting to Build problem.
I0706 19:34:02.476633 138999015860032 graph.py:557] 
I0706 19:34:02.476660 138999015860032 graph.py:558] a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b; e = on_line e a c, on_line e b d ? perp a d b c
I0706 19:34:02.477185 138999015860032 graph.py:581] True
I0706 19:34:02.478700 138999015860032 ddar.py:95] Level 1/1000
I0706 19:34:02.486976 138999015860032 ddar.py:60] Depth 1/1000 time = 0.008189201354980469
I0706 19:34:02.497992 138999015860032 ddar.py:60] Depth 2/1000 time = 0.010838031768798828
I0706 19:34:02.511063 138999015860032 ddar.py:60] Depth 3/1000 time = 0.012906789779663086
I0706 19:34:07.189234 138999015860032 alphageometry.py:641] Solved.
Problem content:  a b c = triangle; d = on_tline b a c, on_tline c a b ? perp a d b c
