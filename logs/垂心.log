/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
2025-07-03 20:31:39.902239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
2025-07-03 20:31:41.063112: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
I0703 20:31:41.064525 129434102056768 inference_utils.py:69] Parsing gin configuration.
I0703 20:31:41.064619 129434102056768 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0703 20:31:41.064783 129434102056768 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0703 20:31:41.064810 129434102056768 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0703 20:31:41.064836 129434102056768 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0703 20:31:41.064856 129434102056768 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0703 20:31:41.064874 129434102056768 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0703 20:31:41.064891 129434102056768 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0703 20:31:41.064911 129434102056768 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0703 20:31:41.064929 129434102056768 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=2
I0703 20:31:41.064946 129434102056768 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0703 20:31:41.064963 129434102056768 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0703 20:31:41.065000 129434102056768 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0703 20:31:41.065113 129434102056768 resource_reader.py:55] Path not found: base_htrans.gin
I0703 20:31:41.065261 129434102056768 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0703 20:31:41.065355 129434102056768 resource_reader.py:55] Path not found: trainer_configuration.gin
I0703 20:31:41.069358 129434102056768 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0703 20:31:41.069501 129434102056768 resource_reader.py:55] Path not found: size/medium_150M.gin
I0703 20:31:41.069739 129434102056768 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0703 20:31:41.069831 129434102056768 resource_reader.py:55] Path not found: options/positions_t5.gin
I0703 20:31:41.070034 129434102056768 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0703 20:31:41.070120 129434102056768 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0703 20:31:41.070407 129434102056768 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0703 20:31:41.070490 129434102056768 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0703 20:31:41.072959 129434102056768 training_loop.py:334] ==== Training loop: initializing model ====
I0703 20:31:41.075024 129434102056768 xla_bridge.py:166] Remote TPU is not linked into jax; skipping remote TPU.
I0703 20:31:41.075098 129434102056768 xla_bridge.py:413] Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'
I0703 20:31:41.075155 129434102056768 xla_bridge.py:413] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0703 20:31:41.075184 129434102056768 xla_bridge.py:413] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0703 20:31:41.075572 129434102056768 xla_bridge.py:413] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
I0703 20:31:41.075636 129434102056768 xla_bridge.py:413] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
W0703 20:31:41.075674 129434102056768 xla_bridge.py:420] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0703 20:31:41.075711 129434102056768 training_loop.py:335] Process 0 of 1
I0703 20:31:41.075741 129434102056768 training_loop.py:336] Local device count = 1
I0703 20:31:41.075769 129434102056768 training_loop.py:337] Number of replicas = 1
I0703 20:31:41.075790 129434102056768 training_loop.py:339] Using random number seed 42
I0703 20:31:41.225021 129434102056768 training_loop.py:359] Initializing the model.
I0703 20:31:41.300578 129434102056768 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.300856 129434102056768 decoder_stack.py:316] dstack: scanning over 1 windows.
I0703 20:31:41.300927 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0703 20:31:41.300963 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0703 20:31:41.300992 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0703 20:31:41.301020 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0703 20:31:41.301052 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0703 20:31:41.301079 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0703 20:31:41.301106 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0703 20:31:41.301131 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0703 20:31:41.301158 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0703 20:31:41.301189 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0703 20:31:41.301216 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0703 20:31:41.301242 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0703 20:31:41.301265 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.301293 129434102056768 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0703 20:31:41.301376 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.301410 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.301432 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.302650 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.306596 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.317389 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.317748 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.320747 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:41.330201 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.330307 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.330358 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.330385 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.330445 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.331130 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.331219 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.331738 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.333643 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.337780 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.338523 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.338599 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.338630 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.338668 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.338807 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.339067 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.339107 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.341346 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.341447 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.343380 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.343463 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.343693 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.352913 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.360936 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.361113 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.361342 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.361439 129434102056768 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0703 20:31:41.361511 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.361540 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.361570 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.362763 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.364479 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.370990 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.371420 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.373034 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:41.376263 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.376343 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.376377 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.376414 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.376473 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.376744 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.376814 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.377054 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.377624 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.379750 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.380074 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.380153 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.380181 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.380219 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.380375 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.380592 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.380639 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.384019 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.384168 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.385855 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.385961 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.386195 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.389052 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.391192 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.391292 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.391502 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.391578 129434102056768 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0703 20:31:41.391648 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.391677 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.391700 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.392732 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.394353 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.400318 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.400661 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.402278 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:41.405256 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.405326 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.405361 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.405389 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.405449 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.405775 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.405862 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.406164 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.406785 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.408240 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.408503 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.408580 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.408610 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.408647 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.408780 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.408976 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.409018 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.411055 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.411142 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.412903 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.412997 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.413235 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.415557 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.417617 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.417727 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.417975 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.418082 129434102056768 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0703 20:31:41.418163 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.418195 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.418217 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.419286 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.420831 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.427148 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.427475 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.520113 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:41.523640 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.523751 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.523791 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.523818 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.523884 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.524160 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.524246 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.524489 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.525084 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.526649 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.526958 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.527034 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.527078 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.527118 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.527257 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.527494 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.527544 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.529808 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.529899 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.531570 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.531654 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.531894 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.534427 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.536611 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.536714 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.536927 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.537015 129434102056768 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0703 20:31:41.537099 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.537131 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.537154 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.538197 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.539869 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.546033 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.546367 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.548127 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:41.551083 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.551164 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.551201 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.551230 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.551285 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.551540 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.551622 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.551865 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.552441 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.553983 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.554293 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.554372 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.554401 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.554440 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.554597 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.554822 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.554861 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.557517 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.557695 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.559419 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.559544 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.559798 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.562182 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.564355 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.564499 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.564694 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.564779 129434102056768 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0703 20:31:41.564852 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.564896 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.564919 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.565993 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.567538 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.573504 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.573871 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.575453 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:41.578504 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.578586 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.578623 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.578670 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.578729 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.578976 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.579054 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.579290 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.579852 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.581318 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.581601 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.581671 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.581700 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.581737 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.581871 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.582069 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.582107 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.584176 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.584275 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.585918 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.586015 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.586246 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.588680 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.590777 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.590891 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.591080 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.591157 129434102056768 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0703 20:31:41.591230 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.591260 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.591291 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.592611 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.594141 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.600026 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.600352 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.601876 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:41.604851 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.604918 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.604951 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.604981 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.605048 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.605290 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.605356 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.605587 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.606124 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.607571 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.607830 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.607908 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.607938 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.607975 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.608105 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.608310 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.608354 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.610370 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.610455 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.612104 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.612191 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.612429 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.614759 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.616766 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.616853 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.617053 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.617128 129434102056768 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0703 20:31:41.617200 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.617229 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.617251 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.618261 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.619816 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.625653 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.625975 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.627890 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:41.630834 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.630915 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.630952 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.630979 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.631035 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.631344 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.631424 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.631660 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.632228 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.633631 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.633891 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.633967 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.633996 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.634032 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.634166 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.634366 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.634405 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.636536 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.636637 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.638261 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.638347 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.638589 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.640854 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.642893 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.642989 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.643170 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.643253 129434102056768 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0703 20:31:41.643333 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.643364 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.643390 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.644528 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.646059 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.651876 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.652179 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.653747 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:41.656712 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.656799 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.656835 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.656863 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.656917 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.657166 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.657242 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.657474 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.658016 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.659423 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.659686 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.659756 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.659786 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.659832 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.659964 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.660154 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.660191 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.662181 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.662272 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.663871 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.663949 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.664171 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.666715 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.668725 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.668818 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.669011 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.669083 129434102056768 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0703 20:31:41.669161 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.669192 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.669215 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.670235 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.671750 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.677442 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.677791 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.679318 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:41.682240 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.682309 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.682362 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.682390 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.682443 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.682681 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.682751 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.682973 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.683522 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.684955 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.685212 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.685281 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.685309 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.685349 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.685489 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.685725 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.685765 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.687729 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.687819 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.689430 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.689510 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.689734 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.692022 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.694040 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.694141 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.694330 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.694404 129434102056768 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0703 20:31:41.694472 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.694509 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.694534 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.695525 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.697025 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.703052 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.703393 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.704947 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:41.708077 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.708147 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.708182 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.708222 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.708278 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.708529 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.708591 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.708876 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.709489 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.710969 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.711244 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.711318 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.711354 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.711391 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.711525 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.711788 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.711852 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.713965 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.714054 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.715667 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.715772 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.716009 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.718365 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.720388 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.720486 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.720678 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.720755 129434102056768 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0703 20:31:41.720826 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.720855 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.720889 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.721900 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.723486 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.729189 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.729519 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.731095 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:41.734052 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.734125 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.734160 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.734187 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.734252 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.734509 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.734582 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.734812 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.735372 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.736847 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.737112 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.737188 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.737218 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.737254 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.737390 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.737580 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.737624 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.739655 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.739748 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.741285 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.741379 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.741614 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.744194 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.746206 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.746304 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.746497 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.746698 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0703 20:31:41.746751 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0703 20:31:41.746782 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0703 20:31:41.746807 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0703 20:31:41.746839 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0703 20:31:41.746864 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0703 20:31:41.746887 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0703 20:31:41.746911 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0703 20:31:41.746936 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0703 20:31:41.746960 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0703 20:31:41.746982 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0703 20:31:41.747005 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0703 20:31:41.747027 129434102056768 decoder_stack.py:344] dstack: Final layernorm.
I0703 20:31:41.749280 129434102056768 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:41.792354 129434102056768 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.792518 129434102056768 decoder_stack.py:333] dstack: autoregressive generator.
I0703 20:31:41.792568 129434102056768 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0703 20:31:41.792636 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.792690 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.792715 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.792752 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.794294 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.800983 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.801332 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.802903 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:41.813912 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.814032 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.814090 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.814121 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.814188 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.814890 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.814998 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.815538 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.816999 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.820174 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.820890 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.820972 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.821002 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.821040 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.821174 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.821251 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.821281 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.823503 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.823590 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.825077 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.825155 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.825228 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.827660 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.829818 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.829910 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.830099 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.830169 129434102056768 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0703 20:31:41.830237 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.830274 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.830298 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.830336 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.831714 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.838203 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.838518 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.840023 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:41.848009 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.848083 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.848118 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.848157 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.848212 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.848459 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.848533 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.848775 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.849247 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.850696 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.850954 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.851023 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.851054 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.851091 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.851229 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.851305 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.851339 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.853536 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.853623 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.855122 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.855215 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.855288 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.857718 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.859902 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.860000 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.860194 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.860267 129434102056768 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0703 20:31:41.860342 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.860372 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.860410 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.860445 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.861892 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.868181 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.868510 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.870053 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:41.878248 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.878367 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.878414 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.878443 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.878521 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.878775 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.878858 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.879112 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.879608 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.881098 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.881365 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.881443 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.881473 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.881511 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.881643 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.881730 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.881765 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.884019 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.884107 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.885657 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.885745 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.885818 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.888241 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.890402 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.890487 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.890686 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.890759 129434102056768 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0703 20:31:41.890828 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.890856 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.890879 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.890921 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.892297 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.898504 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.898810 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.900336 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:41.908190 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.908262 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.908298 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.908329 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.908382 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.908637 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.908701 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.908926 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.909410 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.910872 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.911121 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.911195 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.911225 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.911261 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.911396 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.911471 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.911510 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.913720 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.913805 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.915308 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.915388 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.915467 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.918009 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.920145 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.920229 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.920425 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.920503 129434102056768 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0703 20:31:41.920574 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.920602 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.920625 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.920659 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.922052 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.928232 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.928532 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.930026 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:41.937909 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.937981 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.938017 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.938045 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.938097 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.938351 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.938415 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.938639 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.939100 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.940569 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.940829 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.940895 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.940930 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.940966 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.941098 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.941170 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.941200 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.943410 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.943496 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.944969 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.945049 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.945118 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.947500 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.949619 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.949702 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.949892 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.949960 129434102056768 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0703 20:31:41.950040 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.950072 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.950095 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.950129 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.951519 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.957858 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.958166 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.959680 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:41.967583 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.967685 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.967724 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.967751 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.967808 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.968051 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.968138 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.968381 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.968849 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.970317 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.970582 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.970651 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:41.970679 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:41.970723 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.970855 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:41.970927 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:41.970957 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.973157 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.973249 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.974756 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.974833 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:41.974902 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:41.977269 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:41.979448 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.979532 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:41.979722 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.979790 129434102056768 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0703 20:31:41.979857 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:41.979897 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:41.979920 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:41.979954 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.981319 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:41.987522 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.987845 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:41.989335 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:41.997363 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:41.997431 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:41.997478 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:41.997506 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.997558 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.997793 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.997868 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.998096 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.998561 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:41.999993 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.000261 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.000335 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.000364 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.000398 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.000536 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.000613 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.000643 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.002860 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.002947 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.004452 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.004528 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.004597 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.006973 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.009103 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.009197 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.009392 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.009463 129434102056768 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0703 20:31:42.009531 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.009560 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.009590 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.009624 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.010993 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.017179 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.017482 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.018978 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.026746 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.026813 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.026845 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.026880 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.026932 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.027148 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.027206 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.027430 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.027906 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.029359 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.029618 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.029687 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.029716 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.029752 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.029887 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.029962 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.029991 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.032129 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.032213 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.033694 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.033775 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.033844 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.036360 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.038474 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.038564 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.038755 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.038822 129434102056768 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0703 20:31:42.038888 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.038917 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.038947 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.038981 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.040336 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.046540 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.046839 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.048330 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.056275 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.056382 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.056422 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.056450 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.056527 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.056787 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.056867 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.057122 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.057632 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.059123 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.059393 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.059472 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.059502 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.059541 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.059677 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.059781 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.059819 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.062125 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.062232 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.063803 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.063911 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.063990 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.066525 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.068754 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.068860 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.069074 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.069157 129434102056768 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0703 20:31:42.069224 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.069253 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.069276 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.069328 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.070751 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.077235 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.077570 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.079107 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.087058 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.087133 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.087167 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.087197 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.087250 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.087516 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.087588 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.087814 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.088306 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.089793 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.090042 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.090118 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.090147 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.090185 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.090318 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.090392 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.090431 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.092628 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.092714 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.094201 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.094282 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.094369 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.096788 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.098959 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.099066 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.099263 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.099366 129434102056768 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0703 20:31:42.099440 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.099468 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.099492 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.099528 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.100980 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.107303 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.107630 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.109143 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.117361 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.117487 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.117527 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.117555 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.117617 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.117895 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.117976 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.118206 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.118725 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.120239 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.120513 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.120584 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.120625 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.120665 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.120802 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.120883 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.120914 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.123199 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.123288 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.124826 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.124902 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.124973 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.127387 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.129540 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.129625 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.129818 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.129886 129434102056768 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0703 20:31:42.129964 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.129995 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.130017 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.130051 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.131455 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.137599 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.137892 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.139393 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.147254 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.147329 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.147364 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.147392 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.147441 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.147668 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.147742 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.147980 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.148449 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.149905 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.150146 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.150213 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.150241 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.150283 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.150418 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.150490 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.150521 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.152672 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.152765 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.154243 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.154323 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.154391 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.156860 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.158973 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.159058 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.159241 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.159319 129434102056768 decoder_stack.py:344] dstack: Final layernorm.
I0703 20:31:42.161010 129434102056768 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:42.206923 129434102056768 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.207039 129434102056768 decoder_stack.py:333] dstack: autoregressive generator.
I0703 20:31:42.207106 129434102056768 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0703 20:31:42.207170 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.207218 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.207243 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.207277 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.208710 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.214938 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.215234 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.216691 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.224121 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.224179 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.224232 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.224270 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.224325 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.224555 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.224619 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.224842 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.225292 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.226693 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.226958 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.227027 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.227056 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.227089 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.227225 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.227300 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.227335 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.229455 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.229537 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.231013 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.231088 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.231156 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.233503 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.235627 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.235717 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.235907 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.235974 129434102056768 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0703 20:31:42.236041 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.236068 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.236099 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.236131 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.237492 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.243659 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.243957 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.245437 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.253043 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.253104 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.253138 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.253165 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.253225 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.253457 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.253519 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.253753 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.254198 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.255786 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.256040 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.256108 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.256136 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.256169 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.256295 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.256383 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.256416 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.258561 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.258643 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.260127 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.260211 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.260283 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.262628 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.264820 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.264924 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.265134 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.265212 129434102056768 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0703 20:31:42.265280 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.265308 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.265338 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.265388 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.266767 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.273062 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.273360 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.274880 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.282644 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.282707 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.282742 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.282771 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.282831 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.283061 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.283118 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.283341 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.283807 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.285249 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.285503 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.285578 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.285608 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.285642 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.285770 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.285842 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.285883 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.287977 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.288061 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.289561 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.289644 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.289717 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.292090 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.294385 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.294467 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.294653 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.294734 129434102056768 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0703 20:31:42.294803 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.294832 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.294855 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.294888 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.296255 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.302529 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.302819 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.304236 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.311626 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.311687 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.311720 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.311746 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.311794 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.312011 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.312073 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.312287 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.312749 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.314122 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.314357 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.314423 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.314458 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.314492 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.314618 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.314687 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.314718 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.316787 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.316866 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.318285 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.318365 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.318445 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.320714 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.322761 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.322839 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.323018 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.323096 129434102056768 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0703 20:31:42.323164 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.323192 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.323214 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.323246 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.324577 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.330712 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.330983 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.332438 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.339826 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.339895 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.339930 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.339956 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.340005 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.340222 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.340277 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.340492 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.340924 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.342289 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.342539 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.342605 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.342632 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.342673 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.342801 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.342870 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.342898 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.344922 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.345011 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.346425 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.346500 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.346567 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.348822 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.350843 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.350922 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.351099 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.351164 129434102056768 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0703 20:31:42.351241 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.351271 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.351293 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.351328 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.352650 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.358574 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.358850 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.360277 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.367651 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.367709 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.367751 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.367778 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.367827 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.368036 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.368100 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.368309 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.368743 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.370289 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.370527 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.370592 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.370620 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.370652 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.370788 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.370858 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.370886 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.372926 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.373016 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.374437 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.374510 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.374575 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.377035 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.379081 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.379164 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.379346 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.379413 129434102056768 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0703 20:31:42.379477 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.379516 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.379538 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.379570 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.380877 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.386743 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.387034 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.388462 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.395870 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.395931 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.395963 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.395989 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.396037 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.396260 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.396330 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.396553 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.396998 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.398379 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.398622 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.398685 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.398712 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.398745 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.398869 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.398938 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.398966 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.400975 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.401054 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.402463 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.402534 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.402600 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.404861 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.406991 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.407072 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.407252 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.407322 129434102056768 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0703 20:31:42.407386 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.407414 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.407436 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.407467 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.408786 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.414721 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.415000 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.416408 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.423832 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.423890 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.423921 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.423946 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.423993 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.424207 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.424262 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.424487 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.424934 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.426295 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.426540 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.426606 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.426632 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.426663 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.426788 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.426857 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.426886 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.428908 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.428987 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.430401 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.430472 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.430538 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.432760 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.434795 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.434877 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.435058 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.435122 129434102056768 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0703 20:31:42.435186 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.435214 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.435235 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.435266 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.436604 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.442576 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.442854 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.444277 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.451672 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.451732 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.451765 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.451791 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.451838 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.452050 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.452111 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.452327 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.452781 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.454134 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.454381 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.454451 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.454477 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.454509 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.454633 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.454703 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.454731 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.456757 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.456836 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.458266 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.458342 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.458408 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.460652 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.462674 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.462754 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.462932 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.462997 129434102056768 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0703 20:31:42.463061 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.463087 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.463108 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.463139 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.464439 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.470329 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.470605 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.472025 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.479393 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.479451 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.479481 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.479506 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.479553 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.479760 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.479816 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.480028 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.480473 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.481980 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.482221 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.482285 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.482312 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.482347 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.482469 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.482537 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.482566 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.484572 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.484653 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.486038 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.486111 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.486177 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.488429 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.490436 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.490516 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.490700 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.490765 129434102056768 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0703 20:31:42.490829 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.490855 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.490877 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.490908 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.492218 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.498124 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.498407 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.499826 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.507176 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.507235 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.507268 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.507293 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.507344 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.507551 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.507614 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.507826 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.508262 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.509616 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.509849 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.509913 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.509941 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.509973 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.510096 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.510164 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.510193 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.512227 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.512307 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.513734 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.513808 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.513873 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.516090 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.518212 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.518292 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.518478 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.518543 129434102056768 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0703 20:31:42.518606 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:42.518633 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:42.518654 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:42.518684 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.519986 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:42.525856 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.526131 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:42.527564 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:42.534946 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:42.535007 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:42.535037 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:42.535062 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.535110 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.535313 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.535373 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.535583 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.536020 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.537369 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.537600 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.537665 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:42.537693 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:42.537725 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.537853 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:42.537925 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:42.537954 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.539982 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.540061 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.541464 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.541536 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:42.541603 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:42.543834 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:42.545849 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.545929 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:42.546107 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:42.546174 129434102056768 decoder_stack.py:344] dstack: Final layernorm.
I0703 20:31:42.547833 129434102056768 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:52.294926 129434102056768 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/flax/optim/base.py:49: DeprecationWarning: Use `optax` instead of `flax.optim`. Refer to the update guide https://flax.readthedocs.io/en/latest/howtos/optax_update_guide.html for detailed instructions.
  warnings.warn(
I0703 20:31:52.479114 129434102056768 training_loop.py:409] No working directory specified.
I0703 20:31:52.479256 129434102056768 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0703 20:31:52.479840 129434102056768 checkpoints.py:429] Restoring checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0703 20:31:54.583217 129434102056768 training_loop.py:447] Only restoring trainable parameters.
I0703 20:31:54.583772 129434102056768 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0703 20:31:54.583858 129434102056768 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.583892 129434102056768 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0703 20:31:54.583920 129434102056768 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0703 20:31:54.583948 129434102056768 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.583973 129434102056768 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.583997 129434102056768 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584020 129434102056768 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584043 129434102056768 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0703 20:31:54.584067 129434102056768 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0703 20:31:54.584089 129434102056768 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584112 129434102056768 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.584135 129434102056768 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0703 20:31:54.584158 129434102056768 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0703 20:31:54.584181 129434102056768 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584204 129434102056768 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.584226 129434102056768 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584254 129434102056768 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584287 129434102056768 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0703 20:31:54.584320 129434102056768 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0703 20:31:54.584350 129434102056768 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584377 129434102056768 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.584403 129434102056768 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0703 20:31:54.584430 129434102056768 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0703 20:31:54.584456 129434102056768 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584482 129434102056768 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.584508 129434102056768 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584534 129434102056768 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584561 129434102056768 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0703 20:31:54.584587 129434102056768 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0703 20:31:54.584613 129434102056768 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584639 129434102056768 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.584666 129434102056768 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0703 20:31:54.584692 129434102056768 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0703 20:31:54.584717 129434102056768 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584743 129434102056768 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.584770 129434102056768 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584796 129434102056768 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584821 129434102056768 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0703 20:31:54.584847 129434102056768 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0703 20:31:54.584873 129434102056768 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.584898 129434102056768 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.584924 129434102056768 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0703 20:31:54.584949 129434102056768 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0703 20:31:54.584975 129434102056768 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585001 129434102056768 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.585028 129434102056768 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585054 129434102056768 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585080 129434102056768 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0703 20:31:54.585105 129434102056768 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0703 20:31:54.585131 129434102056768 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585157 129434102056768 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.585183 129434102056768 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0703 20:31:54.585209 129434102056768 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0703 20:31:54.585234 129434102056768 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585260 129434102056768 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.585286 129434102056768 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585313 129434102056768 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585343 129434102056768 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0703 20:31:54.585369 129434102056768 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0703 20:31:54.585394 129434102056768 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585421 129434102056768 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.585446 129434102056768 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0703 20:31:54.585472 129434102056768 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0703 20:31:54.585498 129434102056768 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585524 129434102056768 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.585549 129434102056768 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585575 129434102056768 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585601 129434102056768 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0703 20:31:54.585627 129434102056768 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0703 20:31:54.585653 129434102056768 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585679 129434102056768 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.585705 129434102056768 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0703 20:31:54.585731 129434102056768 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0703 20:31:54.585757 129434102056768 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585783 129434102056768 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.585809 129434102056768 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585834 129434102056768 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585860 129434102056768 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0703 20:31:54.585886 129434102056768 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0703 20:31:54.585912 129434102056768 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.585938 129434102056768 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.585964 129434102056768 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0703 20:31:54.585990 129434102056768 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0703 20:31:54.586016 129434102056768 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586043 129434102056768 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.586070 129434102056768 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586096 129434102056768 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586122 129434102056768 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0703 20:31:54.586147 129434102056768 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0703 20:31:54.586172 129434102056768 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586197 129434102056768 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.586223 129434102056768 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0703 20:31:54.586249 129434102056768 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0703 20:31:54.586275 129434102056768 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586301 129434102056768 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.586332 129434102056768 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586359 129434102056768 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586385 129434102056768 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0703 20:31:54.586410 129434102056768 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0703 20:31:54.586436 129434102056768 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586462 129434102056768 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.586487 129434102056768 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0703 20:31:54.586512 129434102056768 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0703 20:31:54.586538 129434102056768 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586564 129434102056768 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.586589 129434102056768 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586614 129434102056768 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586639 129434102056768 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0703 20:31:54.586664 129434102056768 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0703 20:31:54.586691 129434102056768 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586716 129434102056768 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.586742 129434102056768 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0703 20:31:54.586767 129434102056768 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0703 20:31:54.586793 129434102056768 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586820 129434102056768 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.586846 129434102056768 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586872 129434102056768 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586898 129434102056768 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0703 20:31:54.586923 129434102056768 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0703 20:31:54.586948 129434102056768 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0703 20:31:54.586973 129434102056768 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0703 20:31:54.586996 129434102056768 training_loop.py:725] Total parameters: 152072288
I0703 20:31:54.587170 129434102056768 training_loop.py:739] Total state size: 0
I0703 20:31:54.683489 129434102056768 training_loop.py:492] Training loop: creating task for mode beam_search
I0703 20:31:54.683742 129434102056768 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0703 20:31:54.683950 129434102056768 training_loop.py:652] Compiling mode beam_search with jit.
I0703 20:31:54.684210 129434102056768 training_loop.py:89] registering functions: dict_keys([])
I0703 20:31:54.687043 129434102056768 graph.py:498] orthocenter
I0703 20:31:54.687124 129434102056768 graph.py:499] a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b ? perp a d b c
I0703 20:31:54.690765 129434102056768 ddar.py:60] Depth 1/1000 time = 0.0015108585357666016
I0703 20:31:54.692743 129434102056768 ddar.py:60] Depth 2/1000 time = 0.001895904541015625
I0703 20:31:54.692977 129434102056768 alphageometry.py:221] DD+AR failed to solve the problem.
I0703 20:31:54.693028 129434102056768 alphageometry.py:539] Depth 0. There are 1 nodes to expand:
I0703 20:31:54.693058 129434102056768 alphageometry.py:543] {S} a : ; b : ; c : ; d : T a b c d 00 T a c b d 01 ? T a d b c {F1} x00
I0703 20:31:54.693085 129434102056768 alphageometry.py:548] Decoding from {S} a : ; b : ; c : ; d : T a b c d 00 T a c b d 01 ? T a d b c {F1} x00
I0703 20:31:54.728189 129434102056768 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.728370 129434102056768 decoder_stack.py:316] dstack: scanning over 1 windows.
I0703 20:31:54.728442 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0703 20:31:54.728486 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0703 20:31:54.728521 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0703 20:31:54.728551 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0703 20:31:54.728580 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0703 20:31:54.728610 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0703 20:31:54.728641 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0703 20:31:54.728670 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0703 20:31:54.728700 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0703 20:31:54.728729 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0703 20:31:54.728758 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0703 20:31:54.728787 129434102056768 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0703 20:31:54.728813 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:54.728843 129434102056768 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0703 20:31:54.728911 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:54.728943 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:54.728967 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:54.729917 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.731393 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:54.737673 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.737992 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:54.739451 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:54.742448 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:54.742513 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:54.742554 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:54.742588 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.742645 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.742885 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.742948 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.743171 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.743704 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.745061 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.745312 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.745385 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:54.745417 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:54.745455 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.745588 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:54.745766 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:54.745805 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.748234 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.748323 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.749858 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.749939 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:54.750158 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:54.752591 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.754778 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.754864 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.755065 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.755138 129434102056768 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0703 20:31:54.755208 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:54.755239 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:54.755265 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:54.756187 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.757613 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:54.763940 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.764234 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:54.765703 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:54.768667 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:54.768731 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:54.768771 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:54.768803 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.768855 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.769078 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.769144 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.769390 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.769920 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.771278 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.771530 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.771602 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:54.771637 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:54.771677 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.771815 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:54.771988 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:54.772028 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.774214 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.774300 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.775819 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.775902 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:54.776121 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:54.778594 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.780760 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.780853 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.781059 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.781134 129434102056768 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0703 20:31:54.781209 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:54.781240 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:54.781267 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:54.782203 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.783654 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:54.790118 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.790434 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:54.791983 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:54.795070 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:54.795136 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:54.795170 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:54.795200 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.795251 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.795488 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.795555 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.795776 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.796302 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.797662 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.797902 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.797968 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:54.797997 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:54.798030 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.798158 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:54.798336 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:54.798375 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.800514 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.800593 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.802108 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.802183 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:54.802393 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:54.804758 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.806890 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.806970 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.807159 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.807227 129434102056768 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0703 20:31:54.807294 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:54.807327 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:54.807350 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:54.808265 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.809815 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:54.815919 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.816210 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:54.817723 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:54.820858 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:54.820924 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:54.820974 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:54.821015 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.821074 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.821322 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.821390 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.821640 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.822193 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.823614 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.823873 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.823945 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:54.823987 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:54.824036 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.824210 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:54.824411 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:54.824458 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.826722 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.826810 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.828362 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.828445 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:54.828680 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:54.831305 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.833532 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.833620 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.833842 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.833915 129434102056768 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0703 20:31:54.833993 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:54.834027 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:54.834049 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:54.834995 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.836498 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:54.842863 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.843167 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:54.844676 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:54.847676 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:54.847740 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:54.847788 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:54.847823 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.847883 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.848127 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.848191 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.848443 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.848999 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.850468 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.850728 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.850802 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:54.850842 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:54.850889 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.851036 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:54.851219 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:54.851261 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.853447 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.853534 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.855041 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.855120 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:54.855355 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:54.857838 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.860008 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.860095 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.860310 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.860410 129434102056768 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0703 20:31:54.860496 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:54.860532 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:54.860564 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:54.861471 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.862903 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:54.869237 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.869551 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:54.871056 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:54.874120 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:54.874184 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:54.874234 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:54.874271 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.874335 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.874579 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.874642 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.874888 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.875452 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.876891 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.877156 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.877229 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:54.877271 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:54.877323 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.877476 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:54.877664 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:54.877708 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.879901 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.879987 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.881534 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.881613 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:54.881864 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:54.884320 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.886489 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.886576 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.886788 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.886862 129434102056768 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0703 20:31:54.886946 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:54.886981 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:54.887012 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:54.887927 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.889384 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:54.895747 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.896044 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:54.897556 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:54.900523 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:54.900585 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:54.900634 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:54.900675 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.900734 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.900977 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.901044 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.901286 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.901834 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.903211 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.903483 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.903557 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:54.903598 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:54.903643 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.903789 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:54.903970 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:54.904012 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.906170 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.906257 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.907759 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.907837 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:54.908061 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:54.910533 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.912688 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.912776 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.912995 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.913073 129434102056768 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0703 20:31:54.913158 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:54.913193 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:54.913224 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:54.914169 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.915598 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:54.922058 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.922362 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:54.923831 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:54.926834 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:54.926897 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:54.926946 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:54.926987 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.927048 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.927296 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.927388 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.927640 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.928212 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.929633 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.929888 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.929961 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:54.930002 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:54.930047 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.930195 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:54.930403 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:54.930449 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.932650 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.932738 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.934258 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.934341 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:54.934571 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:54.936959 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.939139 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.939225 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.939463 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.939542 129434102056768 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0703 20:31:54.939627 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:54.939663 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:54.939695 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:54.940587 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.942006 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:54.948473 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.948822 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:54.950393 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:54.953541 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:54.953612 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:54.953663 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:54.953705 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.953772 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.954032 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.954105 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.954365 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.954934 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.956393 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.956686 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.956763 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:54.956805 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:54.956852 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.957001 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:54.957203 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:54.957245 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.959527 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.959614 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.961152 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.961230 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:54.961477 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:54.963956 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:54.966345 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.966436 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:54.966655 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.966734 129434102056768 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0703 20:31:54.966820 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:54.966857 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:54.966891 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:54.967845 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:54.969335 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.072861 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.073288 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.074947 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:55.078186 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.078279 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.078335 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.078376 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.078444 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.078694 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.078761 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.079009 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.079588 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.081064 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.081348 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.081424 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.081466 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.081514 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.081661 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.081855 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.081897 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.084091 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.084179 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.085696 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.085776 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.086014 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.088495 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.090724 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.090813 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.091030 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.091106 129434102056768 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0703 20:31:55.091193 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.091228 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.091260 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.092188 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.093638 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.100189 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.100590 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.102130 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:55.105211 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.105282 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.105320 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.105372 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.105436 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.105674 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.105747 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.105972 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.106516 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.108130 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.108428 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.108501 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.108528 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.108563 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.108690 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.108878 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.108913 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.111122 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.111206 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.112725 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.112798 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.113008 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.115505 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.117720 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.117804 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.117995 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.118062 129434102056768 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0703 20:31:55.118127 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.118155 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.118178 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.119091 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.120545 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.126837 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.127122 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.128670 129434102056768 transformer_layer.py:213] tlayer: windowed attention.
I0703 20:31:55.131704 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.131765 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.131799 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.131827 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.131876 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.132103 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.132161 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.132380 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.132897 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.134257 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.134512 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.134577 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.134605 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.134638 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.134762 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.134929 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.134964 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.137151 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.137233 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.138743 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.138817 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.139021 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.141386 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.143554 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.143635 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.143823 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.143990 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0703 20:31:55.144034 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0703 20:31:55.144063 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0703 20:31:55.144090 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0703 20:31:55.144113 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0703 20:31:55.144136 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0703 20:31:55.144159 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0703 20:31:55.144182 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0703 20:31:55.144205 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0703 20:31:55.144228 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0703 20:31:55.144251 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0703 20:31:55.144276 129434102056768 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0703 20:31:55.144298 129434102056768 decoder_stack.py:344] dstack: Final layernorm.
I0703 20:31:55.146004 129434102056768 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0703 20:31:55.185934 129434102056768 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.186032 129434102056768 decoder_stack.py:333] dstack: autoregressive generator.
I0703 20:31:55.186109 129434102056768 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0703 20:31:55.186173 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.186201 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.186222 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.186253 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.187630 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.193599 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.193879 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.195278 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.203370 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.203488 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.203528 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.203556 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.203620 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.203855 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.203928 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.204162 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.204651 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.206113 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.206389 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.206462 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.206491 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.206530 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.206662 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.206739 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.206768 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.209074 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.209194 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.210812 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.210901 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.210971 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.213452 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.215680 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.215790 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.215987 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.216069 129434102056768 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0703 20:31:55.216136 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.216165 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.216188 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.216223 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.217623 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.224119 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.224530 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.226097 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.234048 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.234170 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.234208 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.234236 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.234299 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.234543 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.234622 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.234836 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.235326 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.236798 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.237052 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.237121 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.237150 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.237188 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.237324 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.237405 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.237435 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.239966 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.240054 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.241627 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.241725 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.241801 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.244431 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.246819 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.246977 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.247190 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.247272 129434102056768 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0703 20:31:55.247349 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.247378 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.247400 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.247436 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.248914 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.255497 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.255883 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.257441 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.265520 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.265665 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.265701 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.265728 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.265794 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.266039 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.266122 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.266360 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.266872 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.268430 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.268710 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.268783 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.268815 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.268855 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.268999 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.269076 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.269105 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.271507 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.271673 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.273547 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.273748 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.273876 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.277284 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.279866 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.280062 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.280300 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.280413 129434102056768 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0703 20:31:55.280498 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.280531 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.280554 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.280590 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.282205 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.290067 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.290537 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.292320 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.302913 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.303056 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.303119 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.303164 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.303246 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.303569 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.303678 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.303972 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.304575 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.306311 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.306705 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.306810 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.306858 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.306918 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.307154 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.307292 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.307354 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.309939 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.310101 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.311692 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.311777 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.311851 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.314538 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.316729 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.316817 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.317010 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.317082 129434102056768 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0703 20:31:55.317150 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.317179 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.317202 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.317239 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.318630 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.325098 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.325448 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.327186 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.335510 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.335618 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.335659 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.335687 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.335747 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.335986 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.336063 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.336291 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.336770 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.338449 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.338721 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.338791 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.338820 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.338857 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.338989 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.339069 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.339100 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.341355 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.341444 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.343009 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.343086 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.343157 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.345678 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.347948 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.348062 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.348284 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.348380 129434102056768 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0703 20:31:55.348470 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.348508 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.348541 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.348588 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.350043 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.356368 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.356738 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.358334 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.366168 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.366267 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.366321 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.366364 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.366436 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.366695 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.366775 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.367037 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.367537 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.369011 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.369305 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.369390 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.369433 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.369485 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.369642 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.369733 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.369769 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.372256 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.372354 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.373894 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.373975 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.374070 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.376513 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.378670 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.378759 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.378983 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.379064 129434102056768 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0703 20:31:55.379150 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.379185 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.379217 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.379262 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.380710 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.386885 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.387218 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.388755 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.396461 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.396523 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.396556 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.396583 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.396632 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.396852 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.396909 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.397122 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.397591 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.399010 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.399251 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.399322 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.399351 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.399386 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.399518 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.399595 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.399626 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.401771 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.401854 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.403346 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.403422 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.403492 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.405886 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.408021 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.408105 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.408298 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.408373 129434102056768 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0703 20:31:55.408439 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.408468 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.408490 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.408524 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.409889 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.416192 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.416488 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.418009 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.425872 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.425958 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.425993 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.426020 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.426076 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.426319 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.426399 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.426632 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.427097 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.428526 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.428783 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.428851 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.428879 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.428915 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.429043 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.429118 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.429147 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.431335 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.431419 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.432935 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.433009 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.433080 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.435432 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.437547 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.437630 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.437821 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.437891 129434102056768 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0703 20:31:55.437956 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.437984 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.438006 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.438040 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.439395 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.445603 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.445899 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.447374 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.455275 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.455348 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.455381 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.455406 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.455458 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.455676 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.455734 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.455945 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.456408 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.457824 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.458068 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.458133 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.458161 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.458195 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.458323 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.458400 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.458431 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.460667 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.460792 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.462358 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.462445 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.462517 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.464891 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.467042 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.467145 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.467349 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.467425 129434102056768 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0703 20:31:55.467493 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.467522 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.467546 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.467582 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.468955 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.475136 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.475462 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.476986 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.484745 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.484819 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.484854 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.484883 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.484935 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.485167 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.485237 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.485467 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.485941 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.487396 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.487647 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.487717 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.487746 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.487780 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.487908 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.487987 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.488019 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.490348 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.490437 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.491931 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.492008 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.492082 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.494453 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.496581 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.496668 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.496861 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.496932 129434102056768 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0703 20:31:55.496999 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.497028 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.497051 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.497085 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.498437 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.504580 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.504879 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.506349 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.514142 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.514217 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.514252 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.514281 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.514338 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.514570 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.514631 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.514855 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.515321 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.516773 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.517024 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.517092 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.517121 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.517157 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.517286 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.517370 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.517402 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.519565 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.519655 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.521157 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.521234 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.521303 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.523716 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.525856 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.525942 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.526136 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.526206 129434102056768 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0703 20:31:55.526274 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.526303 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.526330 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.526365 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.527745 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.534071 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.534375 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.535892 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.543811 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.543896 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.543931 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.543959 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.544016 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.544251 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.544320 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.544548 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.545011 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.546451 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.546700 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.546767 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.546796 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.546831 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.546957 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.547035 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.547066 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.549204 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.549289 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.550802 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.550880 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.550951 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.553294 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.555417 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.555503 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.555694 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.555769 129434102056768 decoder_stack.py:344] dstack: Final layernorm.
I0703 20:31:55.557481 129434102056768 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0703 20:31:55.601786 129434102056768 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.601973 129434102056768 decoder_stack.py:333] dstack: autoregressive generator.
I0703 20:31:55.602026 129434102056768 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0703 20:31:55.602100 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.602133 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.602156 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.602194 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.603931 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.611165 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.611556 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.613130 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.621376 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.621458 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.621493 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.621520 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.621578 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.621821 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.621887 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.622137 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.622702 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.624245 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.624509 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.624579 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.624608 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.624644 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.624773 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.624850 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.624880 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.627101 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.627187 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.628840 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.628926 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.629011 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.631503 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.633712 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.633805 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.634003 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.634075 129434102056768 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0703 20:31:55.634150 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.634194 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.634222 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.634258 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.635967 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.642881 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.643209 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.644763 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.652746 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.652826 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.652861 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.652890 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.652950 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.653195 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.653263 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.653506 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.653992 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.655480 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.655737 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.655806 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.655835 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.655872 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.655999 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.656074 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.656104 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.658294 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.658384 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.659955 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.660037 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.660112 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.662577 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.664760 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.664849 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.665046 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.665129 129434102056768 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0703 20:31:55.665199 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.665231 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.665254 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.665288 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.666756 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.673255 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.673560 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.675111 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.683270 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.683349 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.683386 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.683413 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.683465 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.683710 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.683777 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.684024 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.684525 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.685978 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.686219 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.686288 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.686322 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.686358 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.686490 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.686570 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.686609 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.688781 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.688877 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.690511 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.690593 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.690665 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.693090 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.695274 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.695369 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.695585 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.695664 129434102056768 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0703 20:31:55.695753 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.695787 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.695812 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.695846 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.697275 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.703822 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.704124 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.705660 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.713660 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.713742 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.713779 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.713807 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.713873 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.714115 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.714186 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.714434 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.714905 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.716400 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.716654 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.716722 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.716751 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.716784 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.716917 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.716994 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.717024 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.719431 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.719523 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.721117 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.721200 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.721274 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.723696 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.725883 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.725977 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.726175 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.726250 129434102056768 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0703 20:31:55.726329 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.726359 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.726382 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.726416 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.727787 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.734156 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.734459 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.735972 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.744292 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.744408 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.744462 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.744505 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.744585 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.744930 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.745035 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.745396 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.746111 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.747884 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.748164 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.748240 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.748270 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.748307 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.748451 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.748527 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.748557 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.750894 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.751016 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.752588 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.752670 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.752742 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.755182 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.757450 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.757541 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.757736 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.757810 129434102056768 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0703 20:31:55.757876 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.757905 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.757928 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.757965 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.759447 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.766087 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.766411 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.767933 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.776797 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.776904 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.776943 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.776971 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.777035 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.777286 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.777373 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.777613 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.778091 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.779627 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.779904 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.780006 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.780037 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.780076 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.780210 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.780294 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.780339 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.782637 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.782736 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.784324 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.784412 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.784493 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.787171 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.789498 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.789597 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.789796 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.789874 129434102056768 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0703 20:31:55.789946 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.789976 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.790000 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.790036 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.791493 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.798054 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.798409 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.799974 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.808140 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.808254 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.808295 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.808328 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.808390 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.808633 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.808706 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.808986 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.809473 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.810952 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.811228 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.811297 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.811330 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.811367 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.811508 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.811588 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.811621 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.813821 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.813905 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.815412 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.815491 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.815562 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.817983 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.820150 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.820237 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.820439 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.820513 129434102056768 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0703 20:31:55.820580 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.820609 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.820633 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.820667 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.822040 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.828344 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.828646 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.830173 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.838119 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.838191 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.838227 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.838254 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.838308 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.838535 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.838595 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.838813 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.839276 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.840738 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.840993 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.841063 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.841096 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.841131 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.841259 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.841337 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.841368 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.843723 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.843811 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.845307 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.845387 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.845458 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.847848 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.850001 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.850090 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.850287 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.850361 129434102056768 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0703 20:31:55.850430 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.850457 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.850480 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.850514 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.851887 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.858091 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.858419 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.859899 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.867652 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.867728 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.867762 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.867790 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.867843 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.868075 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.868146 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.868382 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.868843 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.870293 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.870551 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.870617 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.870646 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.870682 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.870806 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.870879 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.870909 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.873060 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.873148 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.874664 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.874741 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.874814 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.877200 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.879337 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.879421 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.879615 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.879685 129434102056768 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0703 20:31:55.879751 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.879780 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.879803 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.879837 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.881191 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.887541 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.887835 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.889307 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.897085 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.897159 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.897195 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.897223 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.897275 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.897509 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.897581 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.897807 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.898263 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.899666 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.899905 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.899971 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.900000 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.900035 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.900163 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.900236 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.900266 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.902395 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.902482 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.903949 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.904024 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.904093 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.906455 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.908573 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.908659 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.908854 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.908925 129434102056768 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0703 20:31:55.908993 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.909023 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.909045 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.909079 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.910450 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.916655 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.916953 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.918449 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.926242 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.926317 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.926351 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.926383 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.926435 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.926662 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.926732 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.926961 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.927428 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.928871 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.929117 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.929185 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.929215 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.929249 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.929381 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.929457 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.929488 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.931617 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.931702 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.933192 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.933269 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.933341 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.935706 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.937810 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.937895 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.938086 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.938155 129434102056768 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0703 20:31:55.938221 129434102056768 transformer_layer.py:154] tlayer: recurrent = False
I0703 20:31:55.938250 129434102056768 transformer_layer.py:155] tlayer: compute_importance = False
I0703 20:31:55.938273 129434102056768 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0703 20:31:55.938308 129434102056768 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.939662 129434102056768 transformer_base.py:161] kvq: pre_attn dropout.
I0703 20:31:55.945804 129434102056768 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.946099 129434102056768 transformer_base.py:194] kvq: normalize keys, queries.
I0703 20:31:55.947556 129434102056768 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0703 20:31:55.955345 129434102056768 transformer_layer.py:299] tlayer: num_windows = 1.
I0703 20:31:55.955446 129434102056768 attention.py:418] Single window, no scan.
I0703 20:31:55.955484 129434102056768 transformer_layer.py:389] tlayer: self-attention.
I0703 20:31:55.955514 129434102056768 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.955571 129434102056768 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.955811 129434102056768 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.955879 129434102056768 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.956107 129434102056768 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.956573 129434102056768 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.958039 129434102056768 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.958298 129434102056768 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.958374 129434102056768 transformer_layer.py:468] tlayer: End windows.
I0703 20:31:55.958404 129434102056768 transformer_layer.py:472] tlayer: final FFN.
I0703 20:31:55.958440 129434102056768 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.958567 129434102056768 transformer_base.py:410] tbase: post-attention MLP.
I0703 20:31:55.958642 129434102056768 nn_components.py:325] mlp: activation = None
I0703 20:31:55.958676 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.961055 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.961138 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.962662 129434102056768 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.962747 129434102056768 transformer_base.py:443] tbase: final FFN
I0703 20:31:55.962820 129434102056768 nn_components.py:320] mlp: hidden 4096, relu
I0703 20:31:55.965244 129434102056768 nn_components.py:329] mlp: final activation = None
I0703 20:31:55.967393 129434102056768 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.967479 129434102056768 nn_components.py:261] mlp: residual
I0703 20:31:55.967675 129434102056768 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:31:55.967752 129434102056768 decoder_stack.py:344] dstack: Final layernorm.
I0703 20:31:55.969507 129434102056768 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0703 20:32:26.523094 129434102056768 alphageometry.py:565] LM output (score=-1.119110): "e : C a c e 02 C b d e 03 ;"
I0703 20:32:26.523274 129434102056768 alphageometry.py:566] Translation: "e = on_line e a c, on_line e b d"

I0703 20:32:26.523340 129434102056768 alphageometry.py:575] Solving: "a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b; e = on_line e a c, on_line e b d ? perp a d b c"
I0703 20:32:26.523418 129434102056768 graph.py:498] 
I0703 20:32:26.523456 129434102056768 graph.py:499] a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b; e = on_line e a c, on_line e b d ? perp a d b c
I0703 20:32:26.532974 129434102056768 ddar.py:60] Depth 1/1000 time = 0.007730245590209961
I0703 20:32:26.543067 129434102056768 ddar.py:60] Depth 2/1000 time = 0.010005950927734375
I0703 20:32:26.555145 129434102056768 ddar.py:60] Depth 3/1000 time = 0.012005805969238281
I0703 20:32:26.556704 129434102056768 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D : Points
BD ⟂ AC [00]
CD ⟂ AB [01]

 * Auxiliary Constructions:
E : Points
A,E,C are collinear [02]
D,B,E are collinear [03]

 * Proof steps:
001. A,E,C are collinear [02] & D,E,B are collinear [03] & BD ⟂ AC [00] ⇒  ∠AEB = ∠DEC [04]
002. A,E,C are collinear [02] & D,E,B are collinear [03] & BD ⟂ AC [00] ⇒  ∠AED = ∠BEC [05]
003. A,E,C are collinear [02] & BD ⟂ AC [00] ⇒  DB ⟂ AE [06]
004. CD ⟂ AB [01] & DB ⟂ AE [06] ⇒  ∠ABD = ∠(DC-AE) [07]
005. E,B,D are collinear [03] & A,E,C are collinear [02] & ∠ABD = ∠(DC-AE) [07] ⇒  ∠ABE = ∠DCE [08]
006. ∠AEB = ∠DEC [04] & ∠ABE = ∠DCE [08] (Similar Triangles)⇒  AE:DE = EB:EC [09]
007. AE:DE = EB:EC [09] & ∠AED = ∠BEC [05] (Similar Triangles)⇒  ∠ADE = ∠BCE [10]
008. AE:DE = EB:EC [09] & ∠AED = ∠BEC [05] (Similar Triangles)⇒  ∠EAD = ∠EBC [11]
009. ∠ADE = ∠BCE [10] & D,B,E are collinear [03] & A,E,C are collinear [02] & ∠EAD = ∠EBC [11] ⇒  AD ⟂ BC
==========================

Traceback (most recent call last):
  File "/home/xmy/miniconda3/envs/AG/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/xmy/miniconda3/envs/AG/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/xmy/alphageometry/alphageometry.py", line 652, in <module>
    app.run(main)
  File "/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xmy/alphageometry/alphageometry.py", line 639, in main
    run_alphageometry(
  File "/home/xmy/alphageometry/alphageometry.py", line 580, in run_alphageometry
    if run_ddar(g_new, p_new, out_file):
  File "/home/xmy/alphageometry/alphageometry.py", line 226, in run_ddar
    gh.nm.draw(
  File "/home/xmy/alphageometry/numericals.py", line 1243, in draw
    plt.show(block=block)
  File "/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/matplotlib/pyplot.py", line 445, in show
    return _get_backend_mod().show(*args, **kwargs)
  File "/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/matplotlib/backend_bases.py", line 3616, in show
    cls.mainloop()
  File "/home/xmy/miniconda3/envs/AG/lib/python3.10/site-packages/matplotlib/backends/_backend_tk.py", line 523, in start_main_loop
    first_manager.window.mainloop()
  File "/home/xmy/miniconda3/envs/AG/lib/python3.10/tkinter/__init__.py", line 1458, in mainloop
    self.tk.mainloop(n)
KeyboardInterrupt
Problem content:  a b c = triangle; d = on_tline b a c, on_tline c a b ? perp a d b c
